
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-QXEE17ZF5T"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-QXEE17ZF5T');
    </script>

    <title>Deceptive Risk Minimization: Out-of-Distribution Generalization by Deceiving Distribution Shift Detectors</title>
    <link rel="icon" type="image/png" href="images/favicon_transparent.png">

    <meta name="description" content="Deceptive Risk Minimization: Out-of-Distribution Generalization by Deceiving Distribution Shift Detectors">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

    <!--FACEBOOK-->
    <meta property="og:image" content="https://deceptive-risk.github.io/images/share_image.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://deceptive-risk.github.io/"/>
    <meta property="og:title" content="Deceptive Risk Minimization" />
    <meta property="og:description" content="Project page for Deceptive Risk Minimization: Out-of-Distribution Generalization by Deceiving Distribution Shift Detectors." />

    <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Deceptive Risk Minimization" />
    <meta name="twitter:description" content="Project page for Deceptive Risk Minimization." />
    <meta name="twitter:image" content="https://deceptive-risk.github.io/images/share_image.png">

    <!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
    <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/default.min.css">
    <link rel="stylesheet" href="css/app.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

    <script src="js/app.js"></script>

	
    <style>
        .nav-pills {
          position: relative;
          display: inline;
        }
        .imtip {
          position: absolute;
          top: 0;
          left: 0;
        }
    </style>
</head>
<body>
    <div class="container" id="main">
        <div class="row mt-4">
            <h2 class="col-md-12 text-center">
                <b>Deceptive Risk Minimization</b>: Out-of-Distribution Generalization<br>by Deceiving Distribution Shift Detectors</br> 
            </h2>
        </div>

        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                <br>
                <li><a href="//irom-lab.princeton.edu/majumdar/">Anirudha Majumdar</a></li>
                <br>
                <br>
                    <a href="https://www.princeton.edu/">
                        <image src="images/PU1line.svg" height="55px"> 
                        <!-- Princeton University                        -->
                    </a>
                </ul>
            </div>
            <!-- <div class="col-md-12 text-center">
                <h4>CoRL 2023, Best Student Paper</h4>
            </div> -->
        </div>

        <div class="row justify-content-md-center">
            <div class="col-md-2 text-center">
                <a href="https://arxiv.org/abs/2502.06575">
                <image src="images/paper_thumbnail.png" height="70px">
                <h4><strong>Paper</strong></h4>
                <!-- <h6>Paper</h6> -->
                </a>
            </div>
        </div>

        <div class="row justify-content-md-center">
            <div class="col-md-10 col-lg-8">
                <div class="col-md-12">
                    <video id="v0" width="100%" preload="metadata" playsinline muted loop autoplay>
                        <source src="videos/anchor_website.mp4" type="video/mp4">
                    </video>
                </div>
                <h3 class="mt-4 mb-2">
                    Deceptive Risk Minimization
                </h3>
                <p class="text-justify">
                    This paper proposes <i>deception</i> as a mechanism for out-of-distribution (OOD) generalization: by learning data representations that make training data <i>appear</i> independent and identically distributed (iid) to an observer, we can identify stable features that eliminate spurious correlations and generalize to unseen domains. We refer to this principle as <i>deceptive risk minimization</i> (DRM) and instantiate it with a practical differentiable objective that simultaneously learns features that eliminate distribution shifts from the perspective of a detector based on conformal martingales while minimizing a task-specific loss. In contrast to domain adaptation or prior invariant representation learning methods, DRM does not require access to test data or a partitioning of training data into a finite number of data-generating domains. We empirically demonstrate the efficacy of DRM on numerical experiments involving imitation learning with covariate shift and classification problems with concept shift. 
                </p>
            </div>
        </div>


        <div class="row justify-content-md-center">
            <div class="col-md-10 col-lg-8">
                <br>
                <h3>
                    An illustrative example
                </h3>
                <p class="text-justify">

                    <p style="text-align:center;">
                        <image src="images/martingale_values.png" width="100%">
                    </p>

                    Consider an imitation learning setting where a human has provided a sequence of examples to a robot demonstrating how to perform a given task. These demonstrations are provided in environments that vary the color of the table <i>slightly</i> in a structured way: a third of the demonstrations are provided with one table-bowl color combination, the next third with a slightly different color combination, and the final third with another. The standard approach to learning a policy in such a setting is empirical risk minimization (ERM): <i>assume</i> that data are iid and learn a mapping from the robot's observations to actions by minimizing a behavior cloning loss on training data. Such a policy performs well when deployed with table colors similar to ones seen during training, but fails with colors that are significantly different (see below for numerical results). 
                </p>
                <p>
                    The starting point for our approach is the observation that the non-iid nature of data in this setting can be <i>inferred from the sequence of training environments</i>. The figure above shows the output from a distribution shift detector based on conformal martingales computed on observations from the training environments. This detector spikes strongly once the table color is changed. Crucially, the detector also spikes when provided with the sequence of latent features from the policy computed via ERM, indicating that the policy's latent representation encodes color information. 
                </p>
                <p>
                Now consider a policy that eliminates the distribution shift from the perspective of an observer who is only presented with latent representations from the policy. Intuitively, the data can be made to "appear iid" by eliminating sensitivity to the table color, which leads to OOD generalization to different table colors.
                </p>
                </div>
        </div>

        <div class="row justify-content-md-center">
            <div class="col-md-10 col-lg-8">
                <h3>
                    Algorithmic Approach
                </h3>
                <p class="text-justify">
                    Our goal is to find features that eliminate the distribution shift between training and test settings. Since the learner is only provided with the sequence of training data, we utilize distribution shifts observed in this data as a proxy. Specifically, we learn features that are stable along the training data sequence --- in the sense that they appear iid to an observer --- while also supporting the minimization of the task-specific loss. 
                </p>
                <p class="text-justify">
                    We formulate this learning mechanism as an adversarial game, which we refer to as <strong>deceptive risk minimization</strong> (DRM). An encoder network learns to generate representations that support minimization of a task-specific loss while simultaneously eliminating distribution shifts from the perspective of a detector presented with a sequence of learned representations. We present a practical instantiation of DRM that utilizes <i>conformal martingales</i> (CMs) for distribution shift detection. Concretely, the CM approach computes a quantity that grows quickly in the presence of distribution shifts, but remains small when data are exchangeable. We derive an end-to-end differentiable loss that penalizes the conformal martingale computed on encoded inputs; this loss serves to train the encoder to learn representations that eliminate distribution shifts from the perspective of the CM-based detector.
                </p>
            </div>
        </div>

        <div class="row justify-content-md-center">
            <div class="col-md-10 col-lg-8">
                <h3>
                    Experiments
                    
                </h3>
                <p>
                </p>
                <p style="text-align:center;">
                        <image src="images/all_results.png" width="100%">
                    </p>
                <p class="text-justify">
                    We evaluate DRM in three sets of experiments involving concept shift and covariate shift. Across all three examples, we find that DRM leads to strong OOD generalization in settings involving spurious correlations or irrelevant distractors. In contrast, standard empirical risk minimization (ERM) fails to generalize. We also compare to invariant risk minimization (IRM), which assumes oracle access to a partitioning of training data into a finite number of domains; we find that DRM is able to match the performance of IRM without requiring such an oracle.
                </p>
                <p> </p>
                </div>
        </div>

        <div class="row justify-content-md-center">
            <div class="col-md-10 col-lg-8">
                <h3>
                    Concept shift: Toy 2D example
                </h3>
                <p class="text-justify">                
                    <p style="text-align:center;">
                        <image src="images/simple_example_decision_boundary.png" width="100%">
                    </p>

                    This is a binary classification task with 2D inputs, where the second input dimension strongly but spuriously correlates with the label. Empirical risk minimization (ERM) latches on to this correlation and learns a classifier that relies heavily on the spuriously correlated input dimension (see the decision boundary for ERM above). However, when the correlation is reversed at test time, the performance of the ERM classifier degrades significantly. In contrast, DRM learns a classifier that relies on the stable feature (the first input dimension) and generalizes well to test data.
                    </p>
                </div>
        </div>

        <div class="row justify-content-md-center">
            <div class="col-md-10 col-lg-8">
                <h3>
                    Concept shift: Colored-MNIST
                </h3>
                <p></p>
                <p style="text-align:center;">
                    <image src="images/colored_mnist_tsne.png" width="100%">
                </p>
                <p class="text-justify">
                    Next, we consider the Colored-MNIST task introduced by Arjovsky et al. (2019). The goal is to classify MNIST images, where the digits have been colored either red or green. Similar to the toy 2D example, the color is assigned in a way that has a strong (but spurious) correlation with the label. As a result, ERM-based methods that only rely on minimizing training loss exploit the color information to make predictions; when the correlation between color and the label is reversed at test time, performance degrades significantly. In contrast, DRM learns to ignore color information, leading to strong OOD generalization. This is illustrated by the t-SNE plot above. ERM clusters data based on color (R / G), while DRM learns to ignore color information and instead clusters data based on labels. 
                </p>
                <p> </p>
                <p> </p>
                </div>
        </div>

        <div class="row justify-content-md-center">
            <div class="col-md-10 col-lg-8">
                <h3>
                    Covariate shift: Imitation learning with distractors
                </h3>
                <div class="col-md-12">
                    <video id="v0" width="100%" preload="metadata" playsinline muted loop autoplay>
                        <source src="videos/imitation_video.mp4" type="video/mp4">
                    </video>
                </div>
                <span style="display:block;"></span>
                <p class="text-justify">
                    We consider an imitation learning setting with covariate shift across environments that the robot is trained and deployed in. The task is to pick up and place a red block into a bowl using observations from an RGB camera. The training data consists of 300 expert demonstrations of pick-and-place locations, which are provided in different environments. A third of the demonstrations are provided with one table-and-bowl color combination, the next third with a slightly different combination, and the final third with another combination. At test time, the bowl and table background color are changed to a novel combination that significantly exaggerates the changes seen during training. This results in the performance of behavior cloning (ERM) collapsing, while DRM maintains nearly the same performance as in-distribution settings.
                </div>
        </div>

        <!-- <div class="row justify-content-md-center">
            <div class="col-md-10 col-lg-8">
                <h3>
                    Citation 
                </h3>
                <a href="https://arxiv.org/abs/2307.01928">[arxiv version]</a>
                <div class="form-group col-md-12">
                    <textarea id="bibtex" class="form-control" rows="8" readonly>
@inproceedings{knowno2023,
    title={Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners},
    author={Ren, Allen Z. and Dixit, Anushri and Bodrova, Alexandra and Singh, Sumeet and Tu, Stephen and Brown, Noah and Xu, Peng and Takayama, Leila and Xia, Fei and Varley, Jake and Xu, Zhenjia and Sadigh, Dorsa and Zeng, Andy and Majumdar, Anirudha},
    booktitle={Proceedings of the Conference on Robot Learning (CoRL)},
    year={2023}
}</textarea>
                </div>
            </div>
        </div> -->
        <div class="row justify-content-md-center mt-4">
            <div class="col-md-10 col-lg-8">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                    This work was supported by the Office of Naval Research (N00014-23-1-2148). The website template is modified from <a href="https://code-as-policies.github.io/">Code as Policies</a>.
                </p>
            </div>
        </div>
    </div>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" crossorigin="anonymous"></script>
    <script>hljs.highlightAll();</script>
</body>
</html>
